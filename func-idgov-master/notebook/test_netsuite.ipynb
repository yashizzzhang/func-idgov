{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msal import ConfidentialClientApplication\n",
    "import pandas as pd\n",
    "import os\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import logging, time\n",
    "\n",
    "def_credential = DefaultAzureCredential()  \n",
    "\n",
    "## connect to Azure Key Vault securely\n",
    "keyVaultName = os.environ[\"KEY_VAULT_NAME\"]\n",
    "KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "kv_client = SecretClient(vault_url=KVUri, credential=def_credential)\n",
    "\n",
    "## Secret Keys for Bolddesk\n",
    "account_id       = kv_client.get_secret('netsuite-account-id').value\n",
    "consumer_key     = kv_client.get_secret('netsuite-consumer-key').value\n",
    "consumer_secret  = kv_client.get_secret('netsuite-consumer-secret').value\n",
    "token_id         = kv_client.get_secret('netsuite-token-id').value\n",
    "token_secret     = kv_client.get_secret('netsuite-token-secret').value\n",
    "\n",
    "\n",
    "## Secret Keys for Azure App Registration\n",
    "tenant_id = kv_client.get_secret('azure-tenant-id').value\n",
    "client_id = kv_client.get_secret('idgov-app-client-id').value\n",
    "client_secret = kv_client.get_secret('idgov-app-client-secret').value\n",
    "\n",
    "## google sheet\n",
    "gcp_dashboard_bot_key = kv_client.get_secret('gcp-dashboard-bot-key').value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "os.chdir('../')\n",
    "\n",
    "# from module.azure_ad import AzureAD\n",
    "from module.netsuite import Netsuite \n",
    "from module.warehouse import Warehouse\n",
    "from module.azure_ad import AzureAD\n",
    "from module.google_sheet import GoogleSheet\n",
    "\n",
    "## Initialize AD and Bolddesk API Module\n",
    "ad = AzureAD(tenant_id, client_id, client_secret)\n",
    "ns = Netsuite(account_id, consumer_key, consumer_secret, token_id, token_secret)\n",
    "gs = GoogleSheet(secret_key=gcp_dashboard_bot_key)\n",
    "\n",
    "## initialize warehouse\n",
    "wh = Warehouse(\n",
    "    server=os.environ[\"DB_SERVER\"],\n",
    "    database=os.environ[\"DB_NAME\"],\n",
    "    credential=def_credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge google sheet employees, and Netsuite data\n",
    "## output back to google sheet\n",
    "consolidated_employee_sheet_id = '1Z1zd1AmOWhGeDKSfuRmwf9lRFN4-Q89_rlHs54ZomGA'\n",
    "output_worksheet_name = 'idgov'\n",
    "input_worksheet_name = 'records'\n",
    "\n",
    "gs = GoogleSheet(secret_key=gcp_dashboard_bot_key)\n",
    "wks = gs.pgsc.open_by_key(consolidated_employee_sheet_id).worksheet_by_title(input_worksheet_name)\n",
    "\n",
    "## prepare left side - google sheet employee records\n",
    "df1 = wks.get_as_df()\\\n",
    "    .loc[:, ['region','company_name','global_emp_id','emp_name','emp_email','manager_email','dept','job_title','cost_center','emp_type', 'hire_date','emp_status']]\\\n",
    "    .query('emp_status==\"Active\"')\n",
    "df1.rename(columns = {\n",
    "    'region': 'emp_region',\n",
    "    'company_name': 'emp_company_name',\n",
    "    'global_emp_id': 'emp_global_id',\n",
    "    'manager_email': 'emp_manager_email',\n",
    "    'dept': 'emp_dept',\n",
    "    'cost_center': 'emp_cost_center',\n",
    "    'hire_date': 'emp_hire_date',\n",
    "    'job_title': 'emp_job_title'\n",
    "}, inplace=True)\n",
    "df1['emp_hire_date']  = pd.to_datetime(df1.emp_hire_date,  format='%d-%b-%Y',errors='coerce')\n",
    "\n",
    "## prep right side - netsuite data\n",
    "df2 = wh.get_table('ns_employees_ad_users').loc[:, ['id','global_empid','email','entityid','giveaccess','title','isinactive','lastmodifieddate','datecreated','regco_name','subsidiary_name','subsidiary_country','supervisor_email','department','ad_userPrincipalName','ad_displayName','ad_employeeId','ad_department','ad_companyName','ad_jobTitle','ad_country','ad_manager_displayName','ad_employeeType','ad_accountEnabled','ad_userType','ad_createdDateTime']]\n",
    "df2.rename(columns = {\n",
    "    'id': 'ns_id',\n",
    "    'global_empid': 'ns_global_id',\n",
    "    'email': 'ns_email',\n",
    "    'entityid': 'ns_entityid',\n",
    "    'giveaccess': 'ns_giveaccess',\n",
    "    'title': 'ns_title',\n",
    "    'isinactive': 'ns_isinactive',\n",
    "    'lastmodifieddate': 'ns_lastmodifieddate',\n",
    "    'regco_name': 'ns_regco_name',\n",
    "    'subsidiary_name': 'ns_subsidiary_name',\n",
    "    'subsidiary_country': 'ns_subsidiary_country',\n",
    "    'supervisor_email': 'ns_supervisor_email',\n",
    "    'department': 'ns_department',\n",
    "    'datecreated': 'ns_datecreated'\n",
    "}, inplace=True)\n",
    "\n",
    "## merge left and right\n",
    "df = pd.merge(df1, df2, left_on='emp_email', right_on=\"ns_email\", how='outer')\n",
    "\n",
    "## Reformating date columns, so that it stays compatible with Looker\n",
    "df['ns_datecreated']      = df.ns_datecreated.dt.strftime('%Y-%m-%d')   ## first convert to string\n",
    "df['ns_lastmodifieddate'] = df.ns_lastmodifieddate.dt.strftime('%Y-%m-%d')\n",
    "df['ad_createdDateTime']  = df.ad_createdDateTime.dt.strftime('%Y-%m-%d')\n",
    "df['emp_hire_date']       = df.emp_hire_date.dt.strftime('%Y-%m-%d')\n",
    "## convert to lower case\n",
    "df['emp_email'] = df.emp_email.str.lower()\n",
    "df['ns_email']  = df.ns_email.str.lower()\n",
    "df['ad_userPrincipalName'] = df.ad_userPrincipalName.str.lower()\n",
    "# df['emp_name'] = df.emp_name.str.lower()\n",
    "# df['ns_entityid'] = df.ns_entityid.str.lower()\n",
    "# df['ad_displayName'] = df.ad_displayName.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge google sheet employees, and Netsuite data\n",
    "## output back to google sheet\n",
    "consolidated_employee_sheet_id = '1Z1zd1AmOWhGeDKSfuRmwf9lRFN4-Q89_rlHs54ZomGA'\n",
    "output_worksheet_name = 'idgov'\n",
    "input_worksheet_name = 'records'\n",
    "\n",
    "gs = GoogleSheet(secret_key=gcp_dashboard_bot_key)\n",
    "wks = gs.pgsc.open_by_key(consolidated_employee_sheet_id).worksheet_by_title(input_worksheet_name)\n",
    "\n",
    "## prepare left side - google sheet employee records\n",
    "df1 = wks.get_as_df()\\\n",
    "    .loc[:, ['region','company_name','notice_period','emp_name','emp_email','manager_email','dept','job_title','cost_center','emp_type', 'hire_date','emp_status']]\\\n",
    "    .query('emp_status==\"Active\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 913 entries, 0 to 951\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   region         913 non-null    object \n",
      " 1   company_name   913 non-null    object \n",
      " 2   notice_period  913 non-null    float64\n",
      " 3   emp_name       913 non-null    object \n",
      " 4   emp_email      913 non-null    object \n",
      " 5   manager_email  913 non-null    object \n",
      " 6   dept           913 non-null    object \n",
      " 7   job_title      913 non-null    object \n",
      " 8   cost_center    913 non-null    object \n",
      " 9   emp_type       913 non-null    object \n",
      " 10  hire_date      913 non-null    object \n",
      " 11  emp_status     913 non-null    object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 92.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.hire_date\n",
    "df1.rename(columns = {\n",
    "    'region': 'emp_region',\n",
    "    'company_name': 'emp_company_name',\n",
    "    'global_emp_id': 'emp_global_id',\n",
    "    'manager_email': 'emp_manager_email',\n",
    "    'dept': 'emp_dept',\n",
    "    'cost_center': 'emp_cost_center',\n",
    "    'hire_date': 'emp_hire_date',\n",
    "    'job_title': 'emp_job_title'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['emp_hire_date']  = pd.to_datetime(df1.emp_hire_date,  format='%d-%b-%Y',errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_employee_sheet_id = '1Z1zd1AmOWhGeDKSfuRmwf9lRFN4-Q89_rlHs54ZomGA'\n",
    "output_worksheet_name = 'idgov'\n",
    "\n",
    "gs = GoogleSheet(secret_key=gcp_dashboard_bot_key)\n",
    "wks = gs.pgsc.open_by_key(consolidated_employee_sheet_id).worksheet_by_title('records')\n",
    "\n",
    "## prepare left side - google sheet employee records\n",
    "df1 = wks.get_as_df()\\\n",
    "    .loc[:, ['region','company_name','global_emp_id','emp_name','emp_email','manager_email','dept','job_title','cost_center','emp_type', 'hire_date','emp_status']]\\\n",
    "    .query('emp_status==\"Active\"')\n",
    "\n",
    "df1.rename(columns = {\n",
    "    'region': 'emp_region',\n",
    "    'company_name': 'emp_company_name',\n",
    "    'global_emp_id': 'emp_global_id',\n",
    "    'manager_email': 'emp_manager_email',\n",
    "    'dept': 'emp_dept',\n",
    "    'cost_center': 'emp_cost_center',\n",
    "    'hire_date': 'emp_hire_date',\n",
    "    'job_title': 'emp_job_title'\n",
    "}, inplace=True)\n",
    "df1['emp_hire_date']  = pd.to_datetime(df1.emp_hire_date,  format='%d-%b-%Y',errors='coerce')\n",
    "\n",
    "## prep right side - netsuite data\n",
    "df2 = wh.get_table('ns_employees_ad_users').loc[:, ['id','global_empid','email','entityid','giveaccess','title','isinactive','lastmodifieddate','datecreated','regco_name','subsidiary_name','subsidiary_country','supervisor_email','department','ad_userPrincipalName','ad_displayName','ad_employeeId','ad_department','ad_companyName','ad_jobTitle','ad_country','ad_manager_displayName','ad_employeeType','ad_accountEnabled','ad_userType','ad_createdDateTime']]\n",
    "df2.rename(columns = {\n",
    "    'id': 'ns_id',\n",
    "    'global_empid': 'ns_global_id',\n",
    "    'email': 'ns_email',\n",
    "    'entityid': 'ns_entityid',\n",
    "    'giveaccess': 'ns_giveaccess',\n",
    "    'title': 'ns_title',\n",
    "    'isinactive': 'ns_isinactive',\n",
    "    'lastmodifieddate': 'ns_lastmodifieddate',\n",
    "    'regco_name': 'ns_regco_name',\n",
    "    'subsidiary_name': 'ns_subsidiary_name',\n",
    "    'subsidiary_country': 'ns_subsidiary_country',\n",
    "    'supervisor_email': 'ns_supervisor_email',\n",
    "    'department': 'ns_department',\n",
    "    'datecreated': 'ns_datecreated'\n",
    "}, inplace=True)\n",
    "\n",
    "## merge left and right\n",
    "df = pd.merge(df1, df2, left_on='emp_email', right_on=\"ns_email\", how='outer')\n",
    "\n",
    "## Reformating date columns, so that it stays compatible with Looker\n",
    "df['ns_datecreated']      = df.ns_datecreated.dt.strftime('%Y-%m-%d')   ## first convert to string\n",
    "df['ns_lastmodifieddate'] = df.ns_lastmodifieddate.dt.strftime('%Y-%m-%d')\n",
    "df['ad_createdDateTime']  = df.ad_createdDateTime.dt.strftime('%Y-%m-%d')\n",
    "df['emp_hire_date']       = df.emp_hire_date.dt.strftime('%Y-%m-%d')\n",
    "\n",
    "## Save To Google Sheet\n",
    "wks = gs.pgsc.open_by_key(consolidated_employee_sheet_id).worksheet_by_title(output_worksheet_name)\n",
    "wks.clear(fields='*')\n",
    "wks.set_dataframe(df, start='A1', escape_formulae=True, extend=True, nan='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "1149    None\n",
       "1150    None\n",
       "1151    None\n",
       "1152    None\n",
       "1153    None\n",
       "Name: ad_userPrincipalName, Length: 1154, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ad_userPrincipalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 nns group elimination\n",
       "1                                 ntl group elimination\n",
       "2                             nera networks (s) pte ltd\n",
       "3                           nera telecommunications ltd\n",
       "4             nera telecommunications (myanmar) co.,ltd\n",
       "5                            nera telecommunications as\n",
       "6                               nera (malaysia) sdn bhd\n",
       "7                              nera infocom (m) sdn bhd\n",
       "8                                     pt nera indonesia\n",
       "9                               nera (thailand) limited\n",
       "10    nera telecommunications holding (thailand) co....\n",
       "11                               nera (philippines) inc\n",
       "12          nera telecommunications (australia) pty ltd\n",
       "13            nera telecommunications (vietnam) co, ltd\n",
       "14             nera telecommunications maroc s.a.r.l au\n",
       "15    nera telecommunications(pakistan) (private) li...\n",
       "16                       nera telecommunications fz-llc\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['region', 'company_name', 'global_emp_id', 'manager_email', 'dept', 'job_title', 'cost_center', 'hire_date'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\azure_functions\\func-idgov\\notebook\\test_netsuite.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/azure_functions/func-idgov/notebook/test_netsuite.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df1 \u001b[39m=\u001b[39m wks\u001b[39m.\u001b[39;49mget_as_df()\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/azure_functions/func-idgov/notebook/test_netsuite.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m.\u001b[39;49mloc[:, [\u001b[39m'\u001b[39;49m\u001b[39mregion\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mcompany_name\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mglobal_emp_id\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39memp_name\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39memp_email\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mmanager_email\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mdept\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mjob_title\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mcost_center\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39memp_type\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhire_date\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39memp_status\u001b[39;49m\u001b[39m'\u001b[39;49m]]\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/azure_functions/func-idgov/notebook/test_netsuite.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39mquery(\u001b[39m'\u001b[39m\u001b[39memp_status==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mActive\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1339\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1337\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:994\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[0;32m    992\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 994\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[0;32m    995\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1380\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1384\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1321\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m )\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1518\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1520\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1522\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6116\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6118\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\azure_functions\\func-idgov\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6177\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6178\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['region', 'company_name', 'global_emp_id', 'manager_email', 'dept', 'job_title', 'cost_center', 'hire_date'] not in index\""
     ]
    }
   ],
   "source": [
    "df1 = wks.get_as_df()\\\n",
    "    .loc[:, ['region','company_name','global_emp_id','emp_name','emp_email','manager_email','dept','job_title','cost_center','emp_type', 'hire_date','emp_status']]\\\n",
    "    .query('emp_status==\"Active\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ns.list_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ns.list_employee_all().loc[:,['id','entityid','email','firstname','lastname','isinactive','title']]\n",
    "e.columns = [ 'owner_' + x for x in e.columns]\n",
    "df = pd.merge( s, e, how='left', left_on='owner', right_on='owner_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ns.list_login_failure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh.erase ('ns_login_failure')\n",
    "wh.append('ns_login_failure', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.script_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ''' \n",
    "    SELECT\n",
    "        ScriptNote.internalId,\n",
    "        ScriptNote.date,\n",
    "        ScriptNote.scriptType as script_id,\n",
    "        case\n",
    "\t\t\twhen script.name IS NOT NULL then 'server'\n",
    "            when clientScript.name IS NOT NULL then 'client'\n",
    "            when suitelet.name IS NOT NULL then 'suitelet'\n",
    "\t        when workflowActionScript.name IS NOT NULL then 'workflowActionScript'\n",
    "            else 'unknown'\n",
    "        end as script_type,\n",
    "        clientScript.name || script.name || suitelet.name || workflowActionScript.name as script_name,\n",
    "        ScriptNote.title,\n",
    "        ScriptNote.detail\n",
    "    FROM\n",
    "        ScriptNote\n",
    "        LEFT OUTER JOIN script ON ScriptNote.scriptType = script.id\t\t\n",
    "        LEFT OUTER JOIN clientScript ON ScriptNote.scriptType= clientScript.id\n",
    "        LEFT OUTER JOIN suitelet ON ScriptNote.scriptType = suitelet.id\n",
    "        LEFT OUTER JOIN workflowActionScript ON ScriptNote.scriptType = workflowActionScript.id\n",
    "    WHERE \n",
    "\t    type = 'ERROR' AND (script.name is not null \n",
    "\t\t\t\t\t\t\tOR clientScript.name is not null \n",
    "\t\t\t\t\t\t\tOR suitelet.name is not null \n",
    "\t\t\t\t\t\t\tOR workflowActionScript.name is not null)\t\n",
    "'''\n",
    "result = ns.query_all(query=q)\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ''' \n",
    "SELECT TOP 100\n",
    "\tSystemNote.context,\n",
    "\tSystemNote.date,\n",
    "\tSystemNote.field,\n",
    "\tSystemNote.id,\n",
    "\tSystemNote.name,\n",
    "\tSystemNote.newValue,\n",
    "\tSystemNote.oldValue,\n",
    "\tSystemNote.record,\n",
    "\tSystemNote.recordId,\n",
    "\tSystemNote.recordTypeId,\n",
    "\tSystemNote.role,\n",
    "\tSystemNote.type\n",
    "FROM\n",
    "\tSystemNote\n",
    "WHERE\n",
    "\tSystemNote.date >= SYSDATE - 90 AND\n",
    "    SystemNote.field NOT LIKE '%MEDIA%'  AND\n",
    "\tSystemNote.recordTypeId = -268\n",
    "'''\n",
    "\n",
    "result = ns.query_all(query=q)\n",
    "n = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ''' \n",
    "SELECT\n",
    "\tSystemNote.role as role, \n",
    "\tSystemNote.recordTypeId as recordTypeId, \n",
    "\tcase \n",
    "\t\twhen MAX(SystemNote.type) = 1 Then 'View' \n",
    "\t\twhen MAX(SystemNote.type) = 2 Then 'Create' \n",
    "\t\twhen MAX(SystemNote.type) = 3 Then 'Edit' \n",
    "\t\telse 'Full' \n",
    "\tend as perm_max\n",
    "FROM\n",
    "\tSystemNote\n",
    "WHERE\n",
    "\tSystemNote.date >= SYSDATE - 90 AND\n",
    "\tSystemNote.field NOT LIKE '%MEDIA%' AND\n",
    "    SystemNote.role IS NOT NULL\n",
    "GROUP BY\n",
    "\tSystemNote.role, SystemNote.recordTypeId\n",
    "'''\n",
    "result = ns.query_all(query=q)\n",
    "df1 = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.list_all_records_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.list_role_record_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_role_ids = list(set(ns.list_employee_roles().role_id.to_list() + ns.list_partner_roles().role_id.to_list()))\n",
    "df = ns.list_role_permissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isin = df.role_id.isin(active_role_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.list_role_permissions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logins Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ns.list_login_audits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'ns_employees_ad_users'\n",
    "users_df    = wh.get_table('ad_users')\\\n",
    "                .drop(columns=['id','passwordProfile_forceChangePasswordNextSignInWithMfa','passwordProfile_forceChangePasswordNextSignIn'])\n",
    "users_df.columns = [ 'ad_'+c for c in users_df.columns]\n",
    "employee_df = ns.list_employees(giveaccess_only=False, refresh=True, active_only=True)\n",
    "df = pd.merge(employee_df, users_df, how='outer', left_on='email', right_on='ad_userPrincipalName')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ns.list_employee_license()\n",
    "df2 = ns.union_employees_partners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_roles_df = ns.list_employee_roles()\n",
    "employee_roles_df['license_type'] = employee_roles_df.role_centertype.apply(lambda x: 'Employee' if x=='EMPLOYEE' else 'Full')\n",
    "cols = ['id','email','entityid','giveaccess','firstname','middlename','lastname','title','issalesrep','issupportrep','isjobresource','isjobmanager','datecreated','lastmodifieddate','isinactive','regco_name','subsidiary_name','subsidiary_country','supervisor_email','department','license_type']\n",
    "employee_roles_df = employee_roles_df[cols].drop_duplicates()\n",
    "# employee_roles_df = r.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approval Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_matrix_df = ns.list_approval_matrix()\n",
    "# employees_df = ns.list_employees().set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_matrix_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_matrix_df['approver_l1_names'] = ''\n",
    "approval_matrix_df['approver_l2_names'] = ''\n",
    "approval_matrix_df['approver_l3_names'] = ''\n",
    "approval_matrix_df['approver_l1_email_names'] = ''\n",
    "approval_matrix_df['approver_l2_email_names'] = ''\n",
    "approval_matrix_df['approver_l3_email_names'] = ''\n",
    "\n",
    "for idx, row in approval_matrix_df.iterrows():\n",
    "\n",
    "    ## L1 Approvers Renaming\n",
    "    if row.approver_l1:\n",
    "        approver_list = [int(x) for x in row.approver_l1.split(',')]\n",
    "        \n",
    "        approver_names=[]\n",
    "        for id in approver_list:\n",
    "            try: \n",
    "                approver_names = approver_names + [employees_df.loc[id, 'entityid']]\n",
    "            except:\n",
    "                approver_names = approver_names + [f'Invalid({str(id)})']\n",
    "        l1_names = ', '.join(approver_names)\n",
    "        approval_matrix_df.loc[idx, 'approver_l1_names'] = l1_names\n",
    "\n",
    "    ## L1_Email Approvers Renaming\n",
    "    if row.approver_l1_email:\n",
    "        approver_list = [int(x) for x in row.approver_l1_email.split(',')]\n",
    "        \n",
    "        approver_names=[]\n",
    "        for id in approver_list:\n",
    "            try: \n",
    "                approver_names = approver_names + [employees_df.loc[id, 'entityid']]\n",
    "            except:\n",
    "                approver_names = approver_names + [f'Invalid({str(id)})']\n",
    "        l1_names = ', '.join(approver_names)\n",
    "        approval_matrix_df.loc[idx, 'approver_l1_email_names'] = l1_names\n",
    "\n",
    "    ## L2 Approvers Renaming\n",
    "    if row.approver_l2:\n",
    "        approver_list = [int(x) for x in row.approver_l2.split(',')]\n",
    "        \n",
    "        approver_names=[]\n",
    "        for id in approver_list:\n",
    "            try: \n",
    "                approver_names = approver_names + [employees_df.loc[id, 'entityid']]\n",
    "            except:\n",
    "                approver_names = approver_names + [f'Invalid({str(id)})']\n",
    "        l1_names = ', '.join(approver_names)\n",
    "        approval_matrix_df.loc[idx, 'approver_l2_names'] = l1_names\n",
    "\n",
    "    ## L2_Email Approvers Renaming\n",
    "    if row.approver_l2_email:\n",
    "        approver_list = [int(x) for x in row.approver_l2_email.split(',')]\n",
    "        \n",
    "        approver_names=[]\n",
    "        for id in approver_list:\n",
    "            try: \n",
    "                approver_names = approver_names + [employees_df.loc[id, 'entityid']]\n",
    "            except:\n",
    "                approver_names = approver_names + [f'Invalid({str(id)})']\n",
    "        l1_names = ', '.join(approver_names)\n",
    "        approval_matrix_df.loc[idx, 'approver_l2_email_names'] = l1_names        \n",
    "    ## L3 Approvers Renaming\n",
    "    if row.approver_l3:\n",
    "        approver_list = [int(x) for x in row.approver_l3.split(',')]\n",
    "        \n",
    "        approver_names=[]\n",
    "        for id in approver_list:\n",
    "            try: \n",
    "                approver_names = approver_names + [employees_df.loc[id, 'entityid']]\n",
    "            except:\n",
    "                approver_names = approver_names + [f'Invalid({str(id)})']\n",
    "        l1_names = ', '.join(approver_names)\n",
    "        approval_matrix_df.loc[idx, 'approver_l3_names'] = l1_names\n",
    "\n",
    "    ## L3_Email Approvers Renaming\n",
    "    if row.approver_l3_email:\n",
    "        approver_list = [int(x) for x in row.approver_l3_email.split(',')]\n",
    "        \n",
    "        approver_names=[]\n",
    "        for id in approver_list:\n",
    "            try: \n",
    "                approver_names = approver_names + [employees_df.loc[id, 'entityid']]\n",
    "            except:\n",
    "                approver_names = approver_names + [f'Invalid({str(id)})']\n",
    "        l1_names = ', '.join(approver_names)\n",
    "        approval_matrix_df.loc[idx, 'approver_l3_email_names'] = l1_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['list_id','list_name','record_id','record_name','description','approver_l1_names','approver_l1_email_names','approver_l2_names','approver_l2_email_names','approver_l3_names','approver_l3_email_names']\n",
    "approval_matrix_df = approval_matrix_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval_matrix_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
